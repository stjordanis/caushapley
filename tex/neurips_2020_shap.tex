\documentclass{article}

\usepackage[nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsfonts}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{wrapfig}
\usetikzlibrary{arrows.meta}

\newcommand{\een}{\mathbb{1}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\ve}{\bm{\epsilon}}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\contribution}{{\phi}}
\newcommand{\val}{{v}}
\newcommand{\dodo}{\mathit{do}}
\newcommand{\ldo}[1]{\dodo(X_{#1} = x_{#1})}
\newcommand{\lvdo}[1]{\dodo(\vX_{#1} = \vx_{#1})}
\newcommand{\sdo}[1]{\hat{x}_{#1}}
\newcommand{\svdo}[1]{\hat{\vx}_{#1}}
\newcommand{\pa}{\mathop{\textit{pa}}}
\newcommand{\spa}{\mathop{\textit{\scriptsize pa}}}
\newcommand{\perm}{\pi}
\newcommand{\actcont}{\contribution^{\mbox{\scriptsize active}}}
\newcommand{\passcont}{\contribution^{\mbox{\scriptsize passive}}}
\newcommand{\operator}{\mathit{op}}
\newcommand{\sop}[1]{\operator(x_{#1})}
\newcommand{\svop}[1]{\operator(\vx_{#1})}
\newcommand{\lop}[1]{\operator(X_{#1} = x_{#1})}
\newcommand{\lvop}[1]{\operator(\vX_{#1} = \vx_{#1})}
\newcommand{\allfeatures}{{N}}
\newcommand{\bx}{\bar{x}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\allmeans}{{\cal X}}
\newcommand{\diagbeta}{{\cal B}}
\newcommand{\mapmat}{{\cal M}}
\newcommand{\contmat}{{\cal C}}
\newcommand{\onder}[2]{{#1}_{\mbox{\scriptsize #2}}}
\newcommand{\boven}[2]{#1^{\mbox{\scriptsize #2}}}
\newcommand{\isequal}{\hspace*{-2.5mm} & = & \hspace*{-2.5mm}}
\newcommand{\chaincomponents}{{\cal T}}
\newcommand{\isequaldo}[1]{\hspace*{-2.5mm} & \overset{(#1)}{=} & \hspace*{-2.5mm}}
\newcommand{\Spre}{\underline{S}}
\newcommand{\Spost}{\bar{S}}

\newcommand{\comment}[1]{{\color{red} #1}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}

\title{Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models} 


\author{%
  Tom Heskes\\
  Radboud University \\
  Institute for Computing and Information Sciences\\
  Nijmegen, The Netherlands \\
  \texttt{tom.heskes@ru.nl} \\
  \And
  Evij Sijben\\
  Radboud University \\
  Institute for Computing and Information Sciences\\
  Nijmegen, The Netherlands \\
  \texttt{e.sijben@student.ru.nl} \\
  \AND
  Ioan Gabriel Bucur \\
  Radboud University \\
  Institute for Computing and Information Sciences\\
  Nijmegen, The Netherlands \\
  \texttt{g.bucur@cs.ru.nl} \\
  \And
  Evij Sijben\\
  Radboud University \\
  Institute for Computing and Information Sciences\\
  Nijmegen, The Netherlands \\
  \texttt{t.claassen@science.ru.nl} \\
}

\begin{document}

\maketitle

\begin{abstract}
Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model's prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a userâ€™s intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated.

In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's \textit{do}-calculus, we show how these `causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example.
\end{abstract}


\section{Introduction}

Complex machine learning models like deep neural networks and ensemble methods like random forest and gradient boosting machines may well outperform simpler approaches such as linear regression or single decision trees, but are noticeably harder to interpret. This can raise practical, ethical, and legal issues, most notably when applied in critical systems, e.g., for medical diagnosis or autonomous driving. The field of explainable AI aims to address these issues by enhancing the interpretability of complex machine learning models.

The Shapley-value approach has quickly become one of the most popular model-agnostic methods within explainable AI. It can provide local explanations, attributing changes in predictions for individual data points to the model's features, that can be combined to obtain better global understanding of the model structure~\cite{lundberg2020local}. Shapley values are based on a principled mathematical foundation~\cite{shapley1953value} and satisfy various desiderata (see also Section~\ref{sec:interpretation}). They have been applied for explaining statistical and machine learning models for quite some time, see e.g.,~\cite{lipovetsky2001analysis,vstrumbelj2014explaining}. Recent interests have been triggered by Lundberg and Lee's breakthrough paper~\cite{lundberg2017unified} that introduces efficient computational procedures and unifies Shapley values and other popular local model-agnostic approaches such as LIME~\cite{ribeiro2016should}.

Humans have a strong tendency to reason about their environment in causal terms~\cite{sloman2005causal}, where explanation and causation are intimately related: explanations often appeal to causes, and causal claims often answer questions about why or how something occurred~\cite{lombrozo2017causal}. The specific domain of causal responsibility studies how people attribute an effect to one or more causes, all of which may have contributed to the observed effect~\cite{sober1988apportioning}. Causal attributions by humans strongly depend on a subject's understanding of the generative model that explains how different causes lead to the effect, for which the relations between these causes are essential~\cite{gerstenberg2012noisy}.


Most explanation methods, however, tend to ignore such relations and act as if features are independent. Even so-called counterfactual approaches, that strongly rely on a causal intuition, make this simplifying assumption (e.g.,~\cite{wachter2017counterfactual}) and ignore that, in the real world, a change in one input feature may cause a change in another. This independence assumption also underlies early Shapley-based approaches, such as~\cite{vstrumbelj2014explaining,datta2016algorithmic}, and is made explicit as an approximation for computational reasons in~\cite{lundberg2017unified}. We will refer to these as {\em marginal} Shapley values.

Aas et al.~\cite{aas2019explaining} argue and illustrate that marginal Shapley values may lead to incorrect explanations when features are highly correlated, motivating what we will refer to as {\em conditional} Shapley values. Janzing et al.~\cite{janzing2019feature}, following~\cite{datta2016algorithmic}, discuss a causal interpretation of Shapley values, in which they replace conventional conditioning by observation with conditioning by intervention, as in Pearl's {\em do}-calculus~\cite{pearl2012calculus}. This, somewhat surprisingly, leads them to conclude that marginal Shapley values are to be preferred over conditional ones. Their argument is also picked up by~\cite{lundberg2020local} when implementing interventional Tree SHAP. Frye et al.~\cite{frye2019asymmetric} propose {\em asymmetric} Shapley values as a way to incorporate causal knowledge by restricting the possible permutations of the features when computing the Shapley values to those consistent with a (partial) causal ordering. In line with~\cite{aas2019explaining}, they then apply conventional conditioning by observation to make sure that the explanations respect the data manifold.

The main contributions of our paper are as follows.
(1)~We derive {\em causal} Shapley values that explain the total effect of features on the prediction, taking into account their causal relationships. This makes them principally different from marginal and conditional Shapley values. Compared to asymmetric Shapley values, causal Shapley values provide a more direct way to incorporate causal knowledge. (2)~Our method allows for further insights into feature relevance by separating out the total causal effect into a direct and indirect contribution. 
(3)~Making use of causal chain graphs~\cite{lauritzen2002chain}, we propose a practical approach for computing causal Shapley values and illustrate this on a real-world example.

\section{A causal interpretation of Shapley values}
\label{sec:interpretation}

In this section, we will introduce our causal, interventional interpretation of Shapley values and contrast this to other approaches. We assume that we are given a machine learning model $f(\cdot)$ that can generate predictions for any feature vector $\vx$. Our goal is to provide an explanation for an individual prediction $f(\vx)$, that takes into account the causal relationships between the features.

Attribution methods, with Shapley values as their most prominent example, provide a local explanation of individual predictions by attributing the difference between $f(\vx)$ and a baseline $f_0$ to the different features $i \in \allfeatures$ with $\allfeatures = \{1,\ldots,n\}$ and $n$ the number of features:
\begin{equation}
f(\vx) = f_0 + \sum_{i=1}^n \contribution_i \: ,
\label{eq:efficiency}
\end{equation}
where $\contribution_i$ is the contribution of feature $i$ to the prediction $f(\vx)$. For the baseline $f_0$ we will take the average prediction $f_0 = \expectation f(\vX)$ with expectation taken over the observed data distribution $P(\vX)$. Equation~(\ref{eq:efficiency}) is referred to as the {\em efficiency property}~\cite{shapley1953value}, which appears to be a sensible desideratum for any attribution method and we therefore take here as our starting point.

To go from knowing none of the feature values, as for $f_0$, to knowing all feature values, as for $f(\vx)$, we can add feature values one by one, actively setting the features to their values in a particular order $\perm$. We define the contribution of feature $i$ given permutation $\perm$ as the difference in value function before and after setting the feature to its value:
\begin{equation}
\contribution_i(\perm) = \val(\{j: j \preceq_\perm i\}) - \val(\{j: j \prec_\perm i\}) \: ,
\label{eq:contperm}
\end{equation}
with $j \prec_\perm i$ if $j$ precedes $i$ in the permutation $\perm$
and where we choose the value function
\begin{equation}
\val(S) = \expectation \left[f(\vX) | \lvdo{S} \right] = \int d\vX_{\bar{S}} \: P(\vX_{\bar{S}}|\lvdo{S}) f(\vX_{\bar{S}},\vx_S) \: .
\label{eq:valuedef}
\end{equation}
Here $S$ is the subset of `in-coalition' indices with known feature values $\vx_S$. To compute the expectation, we average over the `out-of-coalition' or dropped features $\vX_{\bar{S}}$ with $\bar{S} = \allfeatures \setminus S$, the complement of $S$. To explicitly take into account that we actively {\em set} the features to their values, we condition `by intervention' for which we resort to Pearl's \textit{do}-calculus~\cite{pearl1995causal}. In words, the contribution $\contribution_i(\perm)$ now measures the relevance of feature $i$ through the (average) prediction obtained if we actively set feature $i$ to its value $x_i$ compared to (the counterfactual situation of) not knowing its value.

Since the sum over features $i$ in~(\ref{eq:contperm}) is telescoping, the efficiency property~(\ref{eq:efficiency}) holds for any permutation $\perm$. Therefore, for any distribution over permutations $w(\perm)$ with $\sum_{\perm} w(\perm) = 1$, the contributions
\begin{equation}
\contribution_i = \sum_{\perm} w(\perm) \contribution_i(\perm)
\label{eq:shapperm}
\end{equation}
still satisfy~(\ref{eq:efficiency}). An obvious choice would be to take a uniform distribution $w(\perm) = 1/n!$. We then arrive at the standard formula for Shapley values (with shorthand $i$ for the singleton $\{i\}$):
\[
\contribution_i = \sum_{S \subseteq \allfeatures\setminus i} \frac{|S|! (n-|S|-1)!}{n!} \left[\val(S \cup i) - \val(S) \right] \: .
\]
Besides efficiency, the Shapley values uniquely satisfy three other desirable properties~\cite{shapley1953value}.
\begin{description}
	\item[Linearity:] for two value functions $\val_1$ and $\val_2$, we have $\contribution_i(\alpha_1 \val_1 + \alpha_2 \val_2) = \alpha_1 \contribution_i(\val_1) + \alpha_2 \contribution_i(\val_2)$. This guarantees that the Shapley value of a linear ensemble of models is a linear combination of the individual models' Shapley values.
	\item[Null player (dummy):] if $\val(S \cup i) = \val(S)$ for all $S \subseteq \allfeatures \setminus i$, then $\contribution_i = 0$. A feature that never contributes to the prediction (directly nor indirectly, see below) receives zero Shapley value.
	\item[Symmetry:] if $\val(S \cup i) = \val(S \cup j)$ for all  $S \subseteq \allfeatures \setminus \{i,j\}$, then $\contribution_i = \contribution_j$. Symmetry holds for marginal, conditional, and causal Shapley values.
\end{description}
Efficiency, linearity, and null player still hold for a non-uniform distribution of permutations as in~\cite{frye2019asymmetric}, but symmetry is then typically lost.

Replacing conditioning by intervention with conventional conditioning by observation, i.e., averaging over $P(\vX_{\bar{S}}|\vx_{S})$ instead of $P(\vX_{\bar{S}}|\lvdo{S})$ in~(\ref{eq:valuedef}), we arrive at the conditional Shapley values of~\cite{aas2019explaining,lundberg2018consistent}. A third option is to ignore the feature values $\vx_S$ and take the unconditional, marginal distribution $P(\vX_{\bar{S}})$, which leads to the marginal Shapley values.

From the outset, our active, interventional interpretation of Shapley values appears to coincide with that in~\cite{datta2016algorithmic,janzing2019feature,lundberg2020local}. However, the construction in Janzing et al.~\cite{janzing2019feature} ignores any dependencies between the features in the real world, by formally distinguishing between true features (corresponding to one of the data points) and the features plugged as input into the model. This leads to the conclusion that, in our notation, $P(\vX_{\bar{S}}|\lvdo{S}) = P(\vX_{\bar{S}})$ for any subset $S$. As a result, any expectation under conditioning by intervention collapses to a marginal expectation and, in the interpretation of~\cite{datta2016algorithmic,janzing2019feature,lundberg2020local}, interventional Shapley values conveniently simplify to marginal Shapley values. As we will see below, marginal Shapley values can only represent direct effects, which makes that `root causes' with strong indirect effects (e.g.\ genetic markers) are ignored in the attribution, which is quite different from how humans tend to attribute causes~\cite{sober1988apportioning}.

When applied to incorporate causal knowledge, the asymmetric Shapley values introduced in~\cite{frye2019asymmetric} choose $w(\perm) \neq 0$ in~(\ref{eq:shapperm}) only for those permutations $\perm$ that are consistent with the causal structure between the features, i.e., are such that a known causal ancestor always precedes its descendants. In a strong causal chain now only the root cause gets all the credit, which is erring on the opposite side of the explanation spectrum compared to completely ignoring it.
This asymmetric idea can be considered orthogonal to the replacement of conditioning by observation with conditioning by intervention. We will therefore refer to the approach of~\cite{frye2019asymmetric} as {\em asymmetric conditional} Shapley values, to contrast them with {\em asymmetric causal} Shapley values that implement both ideas.

\section{Decomposing Shapley values into direct and indirect effects}

Our causal interpretation allows us to distinguish between direct and indirect effects of each feature on a model's prediction. This is most easily seen by going back to the contribution $\contribution_i(\perm)$ for a permutation $\perm$ and feature $i$ in (\ref{eq:contperm}). With shorthand notation $\Spre = \{j: j \prec_\perm i\}$ and $\Spost = \{j: j \succ_\perm i\}$, we can decompose the total effect for this permutation into a direct and an indirect effect:
\begin{eqnarray*}
	\contribution_i(\perm) \isequal
	\expectation[f(\vX_{\Spost},\vx_{\Spre \cup i})|\lvdo{\Spre \cup i}] - \expectation[f(\vX_{\Spost \cup i},\vx_{\Spre})|\lvdo{\Spre}] ~~~~~~\mbox{(total effect)} \\
	\isequal \expectation[f(\vX_{\Spost},\vx_{\Spre \cup i})|\lvdo{\Spre}] - \expectation[f(\vX_{\Spost \cup i},\vx_{\Spre})|\lvdo{\Spre}] + ~~~~~~~\mbox{(direct effect)} \\
	&& \! \! \! \! \expectation[f(\vX_{\Spost},\vx_{\Spre \cup i})|\lvdo{\Spre \cup i}] - \expectation[f(\vX_{\Spost},\vx_{\Spre \cup i})|\lvdo{\Spre}] ~~~~\mbox{(indirect effect)}
\end{eqnarray*}
The direct effect measures the expected change in prediction when the stochastic feature $X_i$ is replaced by its feature value $x_i$, without changing the distribution of the other `out-of-coalition' features. The indirect effect measures the difference in expectation when the distribution of the other `out-of-coalition' features changes due to the additional intervention $\ldo{i}$. The direct and indirect parts of Shapley values can then be computed as in~(\ref{eq:shapperm}): by taking a, possibly weighted, average over all permutations. Conditional Shapley values can be decomposed similarly. For marginal Shapley values, there is no conditioning and hence no indirect effect: by construction marginal Shapley values can only represent direct effects. We will make use of this decomposition in the next section to clarify how different causal structures lead to different Shapley values.

\section{Shapley values for different causal structures}
\label{sec:toymodels}

\newcommand{\patd}{{\em D}}
\newcommand{\pats}{{\em E}}
\newcommand{\pata}{{\em R}}

\begin{figure}
	\centering
	\begin{tabular}{c|cc|cc|cc}
		& \multicolumn{2}{c|}{\patd} & \multicolumn{2}{c|}{\pats} & \multicolumn{2}{c}{\pata} \\[0.3em] 
		& direct & indirect & direct & indirect & direct & indirect \\ \midrule
		$\phi_1$ & 0 & 0 & 0 & ${1 \over 2} \beta \alpha x_1$ & 0 & $\beta \alpha x_1$ \\
		$\phi_2$ & $\beta x_2$ & 0 & $\beta x_2 - {1 \over 2} \beta \alpha x_1$ & 0 & $\beta x_2 - \beta \alpha x_1$ & 0 \\ \bottomrule
	\end{tabular}\\
	\vspace*{2em}
	\begin{minipage}{0.45\textwidth}
		\centering
		\tikzstyle{arrow} = [thick,->,>=stealth]
		\tikzstyle{dashedarrow} = [thick,->,>=stealth,dashed]
		\tikz{
			% causal chain
			\node[latent] at (0,0) (y1) {$Y$};
			\node[latent] at (0,1.5) (x12) {$X_2$};
			\node[latent] at (0,3) (x11) {$X_1$};
			\node[text height=1em, anchor=north] at (0,4.2) {Chain};
			\draw[arrow] (x12) -- node[anchor=west]{$\beta$} (y1);
			\draw[arrow] (x11) -- node[anchor=west]{$\alpha$} (x12);
			\draw[dashedarrow] (x11) to[bend right] (y1);
			% fork
			\node[latent] at (1.5,0) (y2) {$Y$};
			\node[latent] at (1.5,1.5) (x22) {$X_2$};
			\node[latent] at (1.5,3) (x21) {$X_1$};
			\node[text height=1em, anchor=north] at (1.5,4.2) {Fork};
			\draw[arrow] (x22) -- node[anchor=west]{$\beta$} (y2);
			\draw[arrow] (x22) -- (x21);
			\draw[dashedarrow] (x21) to[bend right] (y2);
			% common confounder
			\node[latent] at (3,0) (y3) {$Y$};
			\node[latent] at (3,1.5) (x32) {$X_2$};
			\node[latent] at (3,3) (x31) {$X_1$};
			\node[latent] at (3.6,2.25) (z)  {$Z$};
			\node[text height=1em, anchor=north] at (3,4.2) {Confounder};
			\draw[arrow] (x32) -- node[anchor=west]{$\beta$} (y3);
			\draw[arrow] (z) -- (x31);
			\draw[arrow] (z) -- (x32);
			\draw[dashedarrow] (x31) to[bend right] (y3);
			% cycle
			\node[latent] at (4.5,0) (y4) {$Y$};
			\node[latent] at (4.5,1.5) (x42) {$X_2$};
			\node[latent] at (4.5,3) (x41) {$X_1$};
			\node[text height=1em, anchor=north] at (4.5,4.2) {Cycle};
			\draw[arrow] (x42) -- node[anchor=west]{$\beta$} (y4);
			\draw[arrow] (x41) to[bend left=15] (x42);
			\draw[arrow] (x42) to[bend left=15] (x41);
			\draw[dashedarrow] (x41) to[bend right] (y4);			
		}
	\end{minipage}
	\hfill
	\begin{minipage}{0.53\textwidth}
		\begin{tabular}{r|c|cc|cc}
			& marginal & \multicolumn{2}{c|}{conditional} & \multicolumn{2}{c}{causal} \\[0.3em] 
			& & \rotatebox{90}{symmetric} & \rotatebox{90}{asymmetric} & \rotatebox{90}{symmetric} & \rotatebox{90}{asymmetric} \\ \midrule
			Chain		& \patd & \pats & \pata & \pats & \pata \\
			Fork		& \patd & \pats & \patd & \patd & \patd \\
			Confounder 	& \patd & \pats & \pats & \patd & \patd \\
			Cycle		& \patd & \pats & \pats & \pats & \pats \\
			\bottomrule
		\end{tabular}
	\end{minipage}
	\caption{Direct and indirect Shapley values for four causal models with the same observational distribution over features (such that $\expectation[X_1] = \expectation[X_2] = 0$ and $\expectation[X_2|x_1] = \alpha x_1$), yet a different causal structure. We assume a linear model that happens to ignore the first feature: $f(x_1,x_2) = \beta x_2$. The bottom table gives for each of the four causal models on the left the marginal, conditional, and causal Shapley values, where the latter two are further split up in symmetric and asymmetric. Each letter in the bottom table corresponds to one of the patterns of direct and indirect effects detailed in the top table: `direct' (\patd, only direct effects), `evenly split' (\pats, credit for an indirect effect split evenly between the features), and `root cause' (\pata, all credit for the indirect effect goes to the root cause).}
	\label{fig:fourmodels}
\end{figure}

To illustrate the difference between the various Shapley values, we consider four causal models on two features. They are constructed such that they have the same $P(\vX)$, with $\expectation[X_2|x_1] = \alpha x_1$ and $\expectation[X_1] = \expectation[X_2] = 0$, but with different causal explanations for the dependency between $X_1$ and $X_2$. We assume to have trained a linear model $f(x_1,x_2)$ that happens to largely, or even completely to simplify the formulas, ignore the first feature, and boils down to the prediction function $f(x_1,x_2) = \beta x_2$. Figure~\ref{fig:fourmodels} shows the explanations provided by the various Shapley values for each of the causal models in this extreme situation. Derivations can be found in the supplement.

To argue which explanations make sense, we call upon classical norm theory~\cite{kahneman1986norm}. It states that humans, when asked for an explanation of an effect, contrast the actual observation with a counterfactual, more normal alternative. What is considered normal, depends on the context. Shapley values can be given the same interpretation~\cite{merrick2019explanation}: they measure the difference in prediction between knowing and not knowing the value of a particular feature, where the choice of what's normal translates to the choice of the reference distribution to average over when the feature value is still unknown.

In this perspective, marginal Shapley values as in~\cite{datta2016algorithmic,janzing2019feature,lundberg2020local} correspond to a very simplistic, counterintuitive interpretation of what's normal. Consider for example the case of the chain, with $X_1$ representing season, $X_2$ temperature, and $Y$ bike rental, and two days with the same temperature of 13 degrees Celsius, one in fall and another in winter. Marginal Shapley values end up with the same explanation for the predicted bike rental on both days, ignoring that the temperature in winter is higher than normal for the time of year and in fall lower. Just like marginal Shapley values, symmetric conditional Shapley values as in~\cite{aas2019explaining} do not distinguish between any of the four causal structures. They do take into account the dependency between the two features, but then fail to acknowledge that an {\em intervention} on feature $X_1$ in the fork and the confounder, does not change distribution of $X_2$.

For the confounder and the cycle, asymmetric Shapley values put $X_1$ and $X_2$ on an equal footing and then coincide with their symmetric counterparts. Asymmetric conditional Shapley values from~\cite{frye2019asymmetric} have no means to distinguish between the cycle and the confounder, unrealistically assigning credit to $X_1$ in the latter case. For the chain and the fork, asymmetric Shapley values only consider the context in which the root cause is set first. This makes that, in our bike rental example of the chain, asymmetric Shapley values first give full credit to season, attributing to temperature only what is left over. Although in general this distribution of credit seems unnecessarily unfair, when dealing with a temporal chain of events, as for example in one of the examples in~\cite{frye2019asymmetric}, it can be argued to align with theories on how humans credit causality in a chain of events~\cite{spellman1997crediting}.

When computing the contribution of, for example, $X_2$, symmetric causal Shapley values always consider two contexts -- one in which $X_1$ is intervened upon before $X_2$ and one in which $X_2$ is intervened upon before $X_1$ -- and then average over the results in these two contexts. This strategy appeals to the theory that humans ``sample counterfactual scenarios''~\cite{icard2017normality} to estimate causal strength, which dates back to~\cite{lewis1974causation}. With the possible exception of asymmetric causal Shapley values for temporal causal structures, the symmetric causal Shapley value are the only ones that give intuitive causal explanations for the total effect of the input features in all four models.

\section{A practical implementation with causal chain graphs}

In the ideal situation, a practitioner has access to a fully specified causal model that can be plugged in~(\ref{eq:valuedef}) to compute or sample from every interventional probability of interest. In practice, such a requirement is hardly realistic. In fact, even if a practitioner could specify a complete causal structure and has full access to the observational probability $P(\vX)$, not every causal query need be identifiable (see e.g., \cite{pearl2012calculus}). Furthermore, requiring so much prior knowledge could be detrimental to the method's general applicability. In this section, we describe a pragmatic approach that is applicable when we have access to a (partial) causal ordering plus a bit of additional information to distinguish confounders from mutual interactions, and a training set to estimate (relevant parameters of) $P(\vX)$.

In the special case that a complete causal ordering of the features can be given and that all causal relationships are unconfounded, $P(\vX)$ satisfies the Markov properties associated with a directed acyclic graph (DAG) and can be written in the form
\[
P(\vX) = \prod_{j \in \allfeatures} P(X_j|\vX_{\spa(j)}) \: ,
\]
with $\pa(j)$ the parents of node $j$. With no further conditional independences, the parents of $j$ are all nodes that precede $j$ in the causal ordering. For causal DAGs, we have the interventional formula~\cite{lauritzen2002chain}:
\begin{equation}
P(\vX_{\bar{S}}|\lvdo{S}) = \prod_{j \in \bar{S}} P(X_j|\vX_{\spa(j)  \cap \bar{S}},\vx_{\spa(j) \cap S}) \: ,
\label{eq:interventional}
\end{equation}
with $\pa(j) \cap T$ the parents of $j$ that are also part of subset $T$. The interventional formula can be used to answer any causal query of interest.

\begin{figure}
	\centering
	\tikz{
		% nodes		
		\node (t1) at (-3,2) [align=left]{partial causal ordering:\\[1em]
		$\left\{(1,2),(3,4,5),(6,7)\right\}$};
		\node (t2) at (-0.5,2) {\Huge $\Rightarrow$};	
		\node (t3) at (4.5,2) {\Huge $\Rightarrow$};		
		\node (1) at (0,4) [circle,draw,inner sep=0.5mm] {\scriptsize $X_1$};
		\node (2) at (1,4) [circle,draw,inner sep=0.5mm] {\scriptsize $X_2$};
		\node (3) at (1.9,2.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_3$};
		\node (4) at (3.1,2.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_4$};
		\node (5) at (2.5,1.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_5$};
		\node (6) at (1,0.5) [circle,draw,inner sep=0.5mm] {\scriptsize $X_6$};
		\node (7) at (2,0.5) [circle,draw,inner sep=0.5mm] {\scriptsize $X_7$};
		\node (8) [draw,label=above:$\tau_1$,inner sep=0.5mm,fit=(1) (2)] {};
		\node (9) [draw,label=above:$\tau_2$,inner sep=0.5mm,fit=(3) (4) (5)] {};
		\node (10) [draw,label=above:$~~\tau_3$,inner sep=0.5mm,fit=(6) (7)] {};
		\draw (1) -- (2);
		\draw (3) -- (4) -- (5) -- (3);
		\draw (6) -- (7);
		\edge {8} {9};
		\edge {8,9} {10};
		\node (11) at (5,4) [circle,draw,inner sep=0.5mm] {\scriptsize $X_1$};
		\node (12) at (6,4) [circle,draw,inner sep=0.5mm] {\scriptsize $X_2$};
		\node (13) at (6.9,2.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_3$};
		\node (14) at (8.1,2.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_4$};
		\node (15) at (7.5,1.8) [circle,draw,inner sep=0.5mm] {\scriptsize $X_5$};
		\node (16) at (6,0.5) [circle,draw,inner sep=0.5mm] {\scriptsize $X_6$};
		\node (17) at (7,0.5) [circle,draw,inner sep=0.5mm] {\scriptsize $X_7$};
		\node (21) at (5.5,4.8) [circle,draw,inner sep=0.7mm]
		{\scriptsize $Z_1$};
		\node (22) at (6.5,-0.3) [circle,draw,inner sep=0.7mm]
		{\scriptsize $Z_2$};
		\node (18) [draw,label=above:$\tau_1$,inner sep=0.5mm,fit=(11) (12) (21)] {};
		\node (19) [draw,label=above:$\tau_2$,inner sep=0.5mm,fit=(13) (14) (15)] {};
		\node (20) [draw,label=above:$~~\tau_3$,inner sep=0.5mm,fit=(16) (17) (22)] {};
		\edge {18} {19};
		\edge {18,19} {20};
		\draw[-Latex] (13) to[bend right=15] (14);
		\draw[-Latex] (14) to[bend right=15] (13);
		\draw[-Latex] (13) to[bend right=15] (15);
		\draw[-Latex] (15) to[bend right=15] (13);
		\draw[-Latex] (15) to[bend right=15] (14);
		\draw[-Latex] (14) to[bend right=15] (15);
		\draw[-Latex] (22) to (16);
		\draw[-Latex] (22) to (17);
		\draw[-Latex] (21) to (11);
		\draw[-Latex] (21) to (12);
	}	
	\caption{From partial ordering to causal chain graph. Features on an equal footing are combined into a fully connected chain component. How to handle interventions within each component depends on the generative process that best explains the (surplus) dependencies. In this example, the dependencies in chain components $\tau_1$ and $\tau_3$ are assumed to be the result of a common confounder, and those in $\tau_2$ of mutual interactions.}
	\label{fig:chaingraph}
\end{figure}

When we cannot give a complete ordering between the individual variables, but still a partial ordering, causal chain graphs~\cite{lauritzen2002chain} come to the rescue. A causal chain graph has directed and undirected edges. All features that are treated on an equal footing are linked together with undirected edges and become part of the same chain component. Edges between chain components are directed and represent causal relationships. See Figure~\ref{fig:chaingraph} for an illustration of the procedure. The probability distribution $P(\vX)$ in a chain graph factorizes as a ``DAG of chain components'':
\[
P(\vX) = \prod_{\tau \in \chaincomponents} P(\vX_\tau|\vX_{\spa(\tau)}) \: ,
\]
with each $\tau$ a chain component, consisting of all features that are treated on an equal footing.

How to compute the effect of an intervention depends on the interpretation of the generative process leading to the (surplus) dependencies between features within each component. If we assume that these are the consequence of marginalizing out a common confounder, intervention on a particular feature will break the dependency with the other features. We will refer to the set of chain components for which this applies as $\onder{\chaincomponents}{confounding}$. The undirected part can also correspond to the equilibrium distribution of a dynamic process resulting from interactions between the variables within a component~\cite{lauritzen2002chain}. In this case, setting the value of a feature does affect the distribution of the variables within the same component. We refer to these sets of components as $\chaincomponents_{\overline{\textrm{\upshape \scriptsize confounding}}}$.

Any expectation by intervention needed to compute the causal Shapley values can be translated to an expectation by observation, by making use of the following theorem (see the supplement for a more detailed proof and some corollaries linking back to other types of Shapley values as special cases).
\begin{theorem}
For causal chain graphs, we have the interventional formula 
\begin{eqnarray}
P(\vX_{\bar{S}}|\lvdo{S}) \isequal \prod_{\tau \in \chaincomponents_{\textrm{\upshape \scriptsize confounding}}} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S}) \times \\ \nonumber
&& \prod_{\tau \in \chaincomponents_{\overline{\textrm{\upshape \scriptsize confounding}}}} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau) \cap \bar{S}},\vx_{\spa(\tau) \cap S},\vx_{\tau \cap S}) \: .
\label{eq:chaininterventional}
\end{eqnarray}
\end{theorem}

\begin{proof}
\begin{eqnarray*}
	P(\vX_{\bar{S}}|\lvdo{S}) \isequaldo{1} \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\lvdo{S}) \hfill \\
	\isequaldo{3}  \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\lvdo{\spa(\tau) \cap S},\lvdo{\tau \cap S}) \\
	\isequaldo{2}  \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S},\lvdo{\tau \cap S}) \: ,
\end{eqnarray*}
where the number above each equal sign refers to the standard \textit{do}-calculus rule from~\cite{pearl2012calculus} that is applied. For a chain component with dependencies induced by a common confounder, rule (3) applies once more and yields $P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S})$,
whereas for a chain component with dependencies induced by mutual interactions, rule (2) again applies and gives $P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{ \spa(\tau) \cap S},\vx_{\tau \cap S}))$.
\end{proof}

To compute these observational expectations, we can rely on the various methods that have been proposed to compute conditional Shapley values~\cite{aas2019explaining,frye2019asymmetric}. Following~\cite{aas2019explaining}, we will assume a multivariate Gaussian distribution for $P(\vX)$ that we estimate from the training data. Alternative proposals include assuming a Gaussian copula distribution, estimating from the empirical (conditional) distribution (both from~\cite{aas2019explaining}) and a variational autoencoder~\cite{frye2019asymmetric}.

\section{Illustration on real-world data}

\begin{figure}[t]
	\centering
	\begin{minipage}{.49\linewidth}
		\includegraphics[width=\textwidth]{figures/trend_plot.pdf}
		\includegraphics[width=\textwidth]{figures/corr_plots_top.pdf}
		\includegraphics[width=\textwidth]{figures/corr_plots_bottom.pdf}
	\end{minipage}
	\begin{minipage}{.5\linewidth}
		\vfill
		\includegraphics[width=\textwidth]{figures/sina_plot.pdf}
	\end{minipage}
	\caption{Bike shares in Washington, D.C.\ in 2011-2012 (top left; colorbar with temperature in degrees Celsius). Sina plot of causal Shapley values for a trained XGBoost model, where the top three date-related variables are considered to be a potential cause of the four weather-related variables (right). Scatter plots of marginal (MSV) versus causal Shapley values (CSV) for temperature ({\em temp}) and one of the seasonal variables ({\em cosyear}) show that MSVs almost purely explain the predictions based on temperature, whereas CSVs also give credit to season (bottom left).}
	\label{fig:trendplot}
\end{figure}

To illustrate the difference between marginal and causal Shapley values, we consider the bike rental dataset from~\cite{fanaee2013bikerental}, where we take as features the number of days since January 2011 ({\em trend}), two cyclical variables to represent season ({\em cosyear}, {\em sinyear}), the temperature ({\em temp}), feeling temperature ({\em atemp}), wind speed ({\em windspeed}), and humidity ({\em hum}). As can be seen from the time series itself (top left plot in Figure~\ref{fig:trendplot}), the bike rental is strongly seasonal and shows an upward trend. Data was randomly split in 80\% training and 20\% test set. We trained an XGBoost model for 100 rounds. 

We adapted the R package SHAPR from~\cite{aas2019explaining} to compute causal Shapley values, which essentially boiled down to an adaptation of the sampling procedure so that it draws samples from the interventional conditional distribution~(\ref{eq:chaininterventional}) instead of from a conventional observational conditional distribution. The sina plot on the righthand side of Figure~\ref{fig:trendplot} shows the causal Shapley values calculated for the trained XGBoost model on the test data. For this simulation, we chose the partial order $(\{\textit{trend}\},\{\textit{cosyear},\textit{sinyear}\},\{\textrm{all weather variables}\})$, with confounding for the second component and no confounding for the third, to represent that season has an effect on weather, but that we have no clue how to represent the intricate relations between the various weather variables. The sina plot clearly shows the relevance of the trend and the season (in particular cosine of the year, which is -1 on January 1 and +1 on July 1). The scatter plots on the left zoom in on the causal (CSV) and marginal Shapley values (MSV) for {\em cosyear} and {\em temp.} The marginal Shapley values for {\em cosyear} vary over a much smaller range than the causal Shapley values for {\em cosyear}, and vice versa for the Shapley values for {\em temp}: where the marginal Shapley values explain the predictions predominantly based on temperature, the causal Shapley values give season much more credit for the higher bike rental in summer and the lower bike rental in winter. A sina plot for the marginal Shapley values can be found in the supplement.

\begin{wrapfigure}[21]{r}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{figures/final_bar_plots.pdf}
	\end{center}
	\caption{Asymmetric (conditional), (symmetric) causal and marginal Shapley values for two different days, one in October (brown) and one in December (gray) with more or less the same temperature of 13 degrees Celsius. Asymmetric Shapley values focus on the root cause, marginal Shapley values on the more direct effect, and symmetric causal Shapley consider both for the more natural explanation.}
	\label{fig:barplots}
\end{wrapfigure}
The difference between asymmetric (conditional, from~\cite{frye2019asymmetric}), (symmetric) causal, and marginal Shapley values clearly shows when we consider two days, October 10 and December 3, 2012, with more or less the same temperature of 13 and 13.27 degrees Celsius, and predicted bike counts of 6117 and 6241, respectively. The temperature and predicted bike counts are relatively low for October, yet high for December. The various Shapley values for {\em cosyear} and {\em temp} are shown in Figure~\ref{fig:barplots}. The marginal Shapley values provide more or less the same explanation for both days, essentially only considering the more direct effect {\em temp}. The asymmetric conditional Shapley values, which are almost indistinguishable from the asymmetric causal Shapley values in this case, put a huge emphasis on the `root' cause {\em cosyear}. The (symmetric) causal Shapley values nicely balance the two extremes, giving credit to both season and temperature, to provide a sensible, but still different explanation for the two days.

\section{Discussion}

In real-world systems, understanding {\em why} things happen typically implies a causal perspective. It means distinguishing between important, contributing factors and irrelevant side effects. Similarly, understanding why a certain instance leads to a given output by a complex algorithm asks for those features that carry a significant amount of information contributing to the final outcome. Our insight was to recognize the need to properly account for the underlying causal structure between the features in order to derive meaningful and relevant attributive properties in the context of a complex algorithm.

For that, this paper introduced causal Shapley values, a model-agnostic approach to split a model's prediction of the target variable for an individual data point into contributions of the features that are used as input to the model, where each contribution aims to estimate the total effect of that feature on the target and can be decomposed into a direct and an indirect effect. We contrasted causal Shapley values with (interventional interpretations of) marginal and (asymmetric variants of) conditional Shapley values. We proposed a novel algorithm to compute these causal Shapley values, based on causal chain graphs. All that a practitioner needs to provide is a partial causal order (as for asymmetric Shapley values) and a way to interpret dependencies between features that are on an equal footing. Existing code for computing conditional Shapley values is easily generalized to causal Shapley values, without additional computational complexity. Computing conditional and causal Shapley values can be considerably more expensive than computing marginal Shapley values due to the need to sample from conditional instead of marginal distributions, even when integrated with computationally efficient approaches such as KernelSHAP~\cite{lundberg2017unified} and TreeExplainer~\cite{lundberg2020local}.

Our approach should be a promising step in providing clear and intuitive explanations for predictions made by a wide variety of complex algorithms, that fits well with natural human understanding and expectations. Additional user studies should confirm to what extent explanations provided by causal Shapley values align with the needs and requirements of practitioners in real-world settings. Similar ideas may also be applied to improve current approaches for (interactive) counterfactual explanations~\cite{wachter2017counterfactual} and properly distinguish between direct and total effects of features on a model's prediction. If successful, causal approaches that better match human intuition may help to build much needed trust in the decisions and recommendations of powerful modern-day algorithms. 


\section*{Broader Impact}




Our research, which aims to provide an explanation for complex machine learning models that can be understood by humans, falls within the scope of explainable AI (XAI). XAI methods like ours can help to open up the infamous ``black box'' of complicated machine learning models like deep neural networks and decision tree ensembles. A better understanding of the predictions generated by such models may provide higher trust~\cite{ribeiro2016should}, detect flaws and biases~\cite{kusner2017counterfactual}, higher accuracy~\cite{bhatt2020explainable}, and even address the legal ``right for an explanation'' as formulated in the GDPR~\cite{gdpr2017}.

Despite their good intentions, explanation methods do come with associated risks. Almost by definition, any sensible explanation of a complex machine learning system involves some simplification and hence must sacrifice some accuracy. It is important to better understand what these limitations are~\cite{kumar2020problems}. Model-agnostic general purpose explanation tools are often applied without properly understanding their limitations and over-trusted~\cite{kaur2020interpreting}. They could possibly even be misused just to check a mark in internal or external audits. Automated explanations can further give an unjust sense of transparency, sometimes referred to as the `transparency fallacy'~\cite{edwards2017slave}: overestimating one's actual understanding of the system. Last but not least, tools for explainable AI are still mostly used as an internal resource by engineers and developers to identify and reconcile errors~\cite{bhatt2020explainable}.

Causality is essential to understanding any process and system, including complex machine learning models. Humans have a strong tendency to reason about their environment and to frame explanations in causal terms~\cite{sloman2005causal,lombrozo2017causal} and causal-model theories fit well to how humans, for example, classify objects~\cite{rehder2003causal}. In that sense, explanation approaches like ours, that appeal to a human's capability for causal reasoning should represent a step in the right direction~\cite{mittelstadt2019explaining}.

\begin{ack}
This research has been partially financed by the Netherlands Organisation for Scientific Research (NWO), under project 617.001.451. 
\end{ack}

\bibliography{shapleyrefs}
\bibliographystyle{plain}



\end{document}
