\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:

\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsfonts}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{txfonts}
\usepackage{stackrel}
\usetikzlibrary{arrows.meta}

\newcommand{\een}{\mathbb{1}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\ve}{\bm{\epsilon}}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\contribution}{{\phi}}
\newcommand{\val}{{v}}
\newcommand{\dodo}{\mathit{do}}
\newcommand{\ldo}[1]{\dodo(X_{#1} = x_{#1})}
\newcommand{\lvdo}[1]{\dodo(\vX_{#1} = \vx_{#1})}
\newcommand{\sdo}[1]{\hat{x}_{#1}}
\newcommand{\svdo}[1]{\hat{\vx}_{#1}}
\newcommand{\pa}{\mathop{\textit{pa}}}
\newcommand{\spa}{\mathop{\textit{\scriptsize pa}}}
\newcommand{\perm}{\pi}
\newcommand{\actcont}{\contribution^{\mbox{\scriptsize active}}}
\newcommand{\passcont}{\contribution^{\mbox{\scriptsize passive}}}
\newcommand{\operator}{\mathit{op}}
\newcommand{\sop}[1]{\operator(x_{#1})}
\newcommand{\svop}[1]{\operator(\vx_{#1})}
\newcommand{\lop}[1]{\operator(X_{#1} = x_{#1})}
\newcommand{\lvop}[1]{\operator(\vX_{#1} = \vx_{#1})}
\newcommand{\allfeatures}{{N}}
\newcommand{\bx}{\bar{x}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\allmeans}{{\cal X}}
\newcommand{\diagbeta}{{\cal B}}
\newcommand{\mapmat}{{\cal M}}
\newcommand{\contmat}{{\cal C}}
\newcommand{\onder}[2]{{#1}_{\mbox{\scriptsize #2}}}
\newcommand{\boven}[2]{#1^{\mbox{\scriptsize #2}}}
\newcommand{\isequal}{\hspace*{-2.5mm} & = & \hspace*{-2.5mm}}
\newcommand{\chaincomponents}{{\cal T}}
\newcommand{\isequaldo}[1]{\hspace*{-2.5mm} & \overset{(#1)}{=} & \hspace*{-2.5mm}}
\newcommand{\Spre}{\underline{S}}
\newcommand{\Spost}{\bar{S}}
\newcommand{\isequaldono}[1]{\overset{(#1)}{=}}

\newcommand{\comment}[1]{{\color{red} #1}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\title{Supplement of ``Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models''} 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\author{%
%  Pietje Puk\\
%  Radboud University
%  Institute for Computing and Information Sciences\\
%  Nijmegen, The Netherlands \\
%  \texttt{pietje.puk@ru.nl} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
%}



\begin{document}

\maketitle

\newcommand{\perpsg}{\stackrel[G]{\sigma}{\Perp}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vZ}{\mathbf{Z}}

\section{\textit{Do}-calculus for cyclic graphs}

For completeness, we here repeat the rules of {\em do}-calculus for cyclic graphs, in the notation of the generalized ID algorithm of ~\cite{forre2019causal}, which generalizes~\cite{pearl2012calculus}. We are given a causal graph $G$. To each node $X_i$ which is intervened upon, we add an `intervention node' $I_{X_i}$, with a directed edge from $I_{X_i}$ to $X_i$ that we clamp to the value $x_i$. The corresponding graph is called $\hat{G}^+$. $\hat{G}_{\dodo(\vW)}$ is now obtained by removing from $\hat{G}^+$ all incoming edges to variables that are part of $\vW$, except those from the corresponding intervention nodes $I_{\vW}$. We use shorthand
\[
\vY \perpsg \vX ~|~ \vZ, \dodo(\mathbf{W})
\]
to indicate that $\vY$ and $\vX$ are $\sigma$-separated by $\vZ$ in the graph $\hat{G}_{\dodo(\vW)}$.  $\sigma$-separation is a generalization of standard d-separation (see~\cite{forre2019causal} for details).

\textit{Do}-calculus now consists of the following three inference rules that can be used to map interventional and observational distributions.
\begin{enumerate}
	\item Insertion/deletion of observation:
	\[
	 P(\vY|\vX,\vZ,\dodo(\vW)) = P(\vY|\vZ,\dodo(\vW)) \mbox{~~if~~} \vY \perpsg \vX ~|~ \vZ,\dodo(\vW) \: .
	\]
	\item Action/observation exchange:
	\[
	P(\vY|\dodo(\vX),\vZ,\dodo(\vW)) = P(\vY|\vX,\vZ,\dodo(\vW)) \mbox{~~if~~} \vY \perpsg I_{\vX} ~|~ \vX, \vZ, \dodo(\vW)  \: .
	\]
	\item Insertion/deletion of actions:
	\[
	P(\vY|\dodo(\vX),\vZ,\dodo(\vW)) = P(\vY|\vZ,\dodo(\vW)) \mbox{~~if~~} \vY \perpsg I_{\vX} ~|~ \vZ,\dodo(\vW) \: .
	\]	
\end{enumerate}
Through consecutive application of these rules, we can try to turn any interventional probability of interest into an observational probability.

\section{Shapley values for linear models}

\begin{figure}
		\centering
		\tikzstyle{arrow} = [thick,->,>=stealth]
		\tikzstyle{dashedarrow} = [thick,->,>=stealth,dashed]
		\tikz{
			% causal chain
			\node[latent] at (0,0) (y1) {$Y$};%
			\node[latent] at (0,1.5) (x12) {$X_2$};
			\node[latent] at (0,3) (x11) {$X_1$};
			\node[text height=1em, anchor=north] at (0,4.2) {Chain and fork};
			\draw[arrow] (x12) -- node[anchor=west]{$\beta_2$} (y1);
			\draw[arrow] (x11) -- (x12);
			\draw[arrow] (x11) to[bend right] node[anchor=east]{$\beta_1$}(y1);
			% common confounder
			\node[latent] at (3,0) (y3) {$Y$};
			\node[latent] at (3,1.5) (x32) {$X_2$};
			\node[latent] at (3,3) (x31) {$X_1$};
			\node[latent] at (3.6,2.25) (z)  {$Z$};
			\node[text height=1em, anchor=north] at (3,4.2) {Confounder};
			\draw[arrow] (x32) -- node[anchor=west]{$\beta_2$} (y3);
			\draw[arrow] (z) -- (x31);
			\draw[arrow] (z) -- (x32);
			\draw[arrow] (x31) to[bend right] node[anchor=east]{$\beta_1$}(y3);
			% cycle
			\node[latent] at (6,0) (y4) {$Y$};%
			\node[latent] at (6,1.5) (x42) {$X_2$};
			\node[latent] at (6,3) (x41) {$X_1$};
			\node[text height=1em, anchor=north] at (6,4.2) {Cycle};
			\draw[arrow] (x42) -- node[anchor=west]{$\beta_2$} (y4);
			\draw[arrow] (x41) to[bend left=15] (x42);
			\draw[arrow] (x42) to[bend left=15] (x41);
			\draw[arrow] (x41) to[bend right] node[anchor=east]{$\beta_1$}(y4);			
		}
	\caption{Three causal models with the same observational distribution over features, yet a different causal structure. To connect to the models in the main text, we set $\beta_1 = 0$ and $\beta_2 = \beta$, except that for the `fork' we set $\beta_2 = 0$, $\beta_1 = \beta$, and then swap the indices.}
	\label{fig:threemodels}
\end{figure}

We will make use of the {\em do}-calculus rules above to derive the causal Shapley values for the four different models in Figure~1 in the main text. To this end, we consider the three models in Figure~\ref{fig:threemodels} that predict $f(x_1,x_2) = \beta_1 x_1 + \beta_2 x_2$ for general values of $\beta_1$ and $\beta_2$. All three models have the same observational probability distribution, with $\expectation[X_i] = \bx_i$ and $\expectation[X_{3-i}|X_i=x_i] = \alpha_i x_i$, for $i = 1,2$, yet different causal structures. We will arrive at the main text's results for the `chain', `confounder', and `cycle' by setting $\beta_1 = 0$, whereas for the `fork' we set $\beta_2 = 0$ and swap the two indices. We then further need to take $\bx_1 = \bx_2 = 0$, and $\alpha = \alpha_2$.

Following the definitions in the main text, the contribution of feature $i$ given permutation $\perm$ is the difference in value function before and after setting the feature to its value:
\begin{equation}
\contribution_i(\perm) = \val(\{j: j \preceq_\perm i\}) - \val(\{j: j \prec_\perm i\}) \: ,
\label{eq:contperm}
\end{equation}
with value function
\begin{equation}
\val(S) = \expectation \left[f(\vX) | \lvdo{S} \right] = \int d\vX_{\bar{S}} \: P(\vX_{\bar{S}}|\svdo{S}) f(\vX_{\bar{S}},\vx_S) \: ,
\label{eq:valuedef}
\end{equation}
where we use shorthand $\svdo{}$ for $\lvdo{}$. Combining these two definitions and substituting $f(\vx) = \sum_i \beta_i x_i$, we obtain
\[
\contribution_i(\perm) =
\beta_i \left(x_i - \expectation [X_i | \svdo{j: j \prec_\perm i}]\right) + \sum_{k \succ_\perm i} \beta_k \left( \expectation [X_k | \svdo{j: j \preceq_\perm i}] - \expectation [X_k | \svdo{j: j \prec_\perm i}] \right) \: .
\]
The first term corresponds to the direct effect, the second one to the indirect effect. Symmetric causal Shapley values will follow by averaging these contributions for the two possible permutations $\perm = (1,2)$ and $\perm = (2,1)$. Conditional Shapley values result when replacing conditioning by intervention with conventional conditioning by observation, marginal Shapley values by not conditioning at all.

To start with the latter, we immediately see that for {\em marginal Shapley values} the indirect effect vanishes and the direct effect simplifies to
\[
\phi_i = \phi_i(\perm) = \beta_i (x_i - \expectation[X_i])  = \beta_i (x_i - \bx_i) \: ,
\]
as also derived in~\cite{aas2019explaining}.

For symmetric conditional Shapley values, we do get different contributions for the two different permutations, but by definition still the same for the three different models:
\begin{eqnarray}
\phi_1(1,2) \isequal \beta_1 (x_1 - \expectation[X_1]) + \beta_2 (\expectation[X_2|x_1] - \expectation[X_2]) = \beta_1 (x_1 - \bx_1) + \beta_2 \alpha_1 (x_1 - \bx_1) \nonumber \\
\phi_2(1,2) \isequal \beta_2 (x_2 - \expectation[X_2|x_1]) = \beta_2 (x_2 - \bx_2) - \beta_2 \alpha_1 (x_1 - \bx_1) \: .
\label{eq:asymmetric}
\end{eqnarray}
Here the first term in the contribution for the first feature corresponds to the direct effect and the second term to the indirect effect. The contribution for the second feature only consists of a direct effect. The contributions for the other permutation follow by swapping the indices and the final Shapley values by averaging to arrive at the {\em symmetric conditional Shapley values}
\begin{eqnarray}
\phi_1 \isequal \beta_1 (x_1 - \bx_1) - {1 \over 2} \beta_1 \alpha_2 (x_2 - \bx_2) + {1 \over 2} \beta_2 \alpha_1 (x_1 - \bx_1) \nonumber \\
\phi_2 \isequal \beta_2 (x_2 - \bx_2) - {1 \over 2} \beta_2 \alpha_1 (x_1 - \bx_1) + {1 \over 2} \beta_1 \alpha_2 (x_2 - \bx_2) \: ,
\label{eq:symmetric}
\end{eqnarray}
where now the first two terms constitute the direct effect and the third term the indirect effect.

\begin{table}
\begin{center}
\begin{tabular}{r|ccc} \toprule
expectation & chain & confounder & cycle \\ \midrule
$\expectation[X_1|\sdo{2}]$ & $\expectation[X_1]$ & $\expectation[X_1]$ & $\expectation[X_1|x_2]$ \\
$\expectation[X_2|\sdo{1}]$ & $\expectation[X_2|x_1]$ & $\expectation[X_2]$ & $\expectation[X_2|x_1]$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{Turning expectations under conditioning by intervention into expectations under conventional conditioning by observation for the three models in Figure~\ref{fig:threemodels}.}
\label{tab:rewriting}
\end{table}

The {\em asymmetric conditional Shapley values} consider both permutations for the confounder and the cycle, and hence are equivalent to the symmetric Shapley values for those models. Yet for the chain, they only consider the permutation $\perm(1,2)$ and thus yield $\bm{\phi} = \bm{\phi}(1,2)$ from~(\ref{eq:asymmetric}).

To go from the symmetric conditional Shapley values to the causal symmetric Shapley values, we follow the same line of reasoning, but have to replace $\expectation[X_2|x_1]$ by $\expectation[X_2|\sdo{1}]$ and $\expectation[X_1|x_2]$ by $\expectation[X_1|\sdo{2}]$. Table~\ref{tab:rewriting} tells whether
the expectations under conditioning by intervention reduce to expectations under conditioning by observation (because of the second rule of \textit{do}-calculus above) or to marginal expectations (because of the third rule). For the chain we have
\[
P(X_2 | \sdo{1}) = P(X_2 | x_1) \mbox{~~since~~} X_2 \perpsg I_{X_1} ~|~ X_1 \mbox{~~(rule 2),~yet~~} P(X_1 | \sdo{2}) = P(X_1) \mbox{~~since~~} X_1 \perpsg I_{X_2} \mbox{~~(rule 3),}
\]
for the confounder
\[
P(X_2 | \sdo{1}) = P(X_2) \mbox{~~since~~} X_2 \perpsg I_{X_1}  \mbox{~~and~~} P(X_1 | \sdo{2}) = P(X_1) \mbox{~~since~~} X_1 \perpsg I_{X_2} \mbox{~~(rule 3),}
\]
and for the cycle
\[
P(X_2 | \sdo{1}) = P(X_2 | x_1) \mbox{~~since~~} X_2 \perpsg I_{X_1} ~|~ X_1 \mbox{~~and~~} P(X_1 | \sdo{2}) = P(X_1|x_2) \mbox{~~since~~} X_1 \perpsg I_{X_2} ~|~ X_2 \mbox{~~(rule 2).}
\]
Consequently, for the confounder the {\em symmetric} and {\em asymmetric causal Shapley values} coincide with the marginal Shapley values (consistent with~\cite{janzing2019feature}) and for the cycle with the symmetric conditional Shapley values from~(\ref{eq:symmetric}). For the chain, the causal symmetric Shapley values become
\begin{eqnarray}
\phi_1(1,2) \isequal \beta_1 (x_1 - \bx_1) + {1 \over 2} \beta_2 \alpha_1 (x_1 - \bx_1) \nonumber \\
\phi_2(1,2) \isequal \beta_2 (x_2 -\bx_2) - {1 \over 2} \beta_2 \alpha_1 (x_1 - \bx_1) \: ,
\label{eq:asymmetric2}
\end{eqnarray}
where the asymmetric causal Shapley values coincides with the asymmetric conditional Shapley values from~(\ref{eq:asymmetric2}).

Collecting all results and setting $\bx_1 = \bx_2 = \beta_1 = 0$, $\beta_2 = \beta$, and $\alpha_1 = \alpha$ (after swapping the indices for the `fork'), we arrive at the Shapley values reported in Figure~1 in the main text. Note that for most Shapley values, the indirect effect for the second feature vanishes because we chose to set $\beta_1 = 0$. The exceptions, apart from the marginal Shapley values, are the causal Shapley values for the chain and the confounder, as well as the asymmetric conditional Shapley values for the chain: these show no indirect effect for the second feature even for nonzero $\beta_1$.

\section{Proofs and corollaries on causal chain graphs}

In this section we expand on the proof of Theorem~1 in the main text and add some corollaries to link back to other approaches for computing Shapley values.

The probability distribution for a causal chain graph boils down to a directed acyclic graph of chain components:
\begin{equation}
P(\vX) = \prod_{\tau \in \chaincomponents} P(\vX_\tau|\vX_{\spa(\tau)}) \: .
\label{eq:chaingraph}
\end{equation}
For each (fully connected) chain component, we further need to specify whether (surplus) dependencies within the component are due to confounding or due to mutual interactions. Given this information, we can turn any causal query into an observational distribution with the following interventional formula.

\begin{theorem}
	For causal chain graphs, we have the interventional formula 
	\begin{eqnarray}
	P(\vX_{\bar{S}}|\lvdo{S}) \isequal \prod_{\tau \in \chaincomponents_{\textrm{\upshape \scriptsize confounding}}} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S}) \times \nonumber \\
	&& \prod_{\tau \in \chaincomponents_{\overline{\textrm{\upshape \scriptsize confounding}}}} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau) \cap \bar{S}},\vx_{\spa(\tau) \cap S},\vx_{\tau \cap S}) \: .
	\label{eq:chaininterventional}
	\end{eqnarray}
\end{theorem}

\begin{proof}
Plugging in~(\ref{eq:chaingraph}) and using shorthand $\svdo{} = \lvdo{}$, we obtain
\[
P(\vX_{\bar{S}}|\svdo{S}) = P(\vX|\svdo{S}) = \prod_{\tau \in \chaincomponents} P(\vX_{\tau}|\vX_{\tau' \prec_G \tau},\svdo{S})
\isequaldono{1} \prod_{\tau \in \chaincomponents} P(\vX_\tau|\vX_{\spa(\tau)},\svdo{S}) = \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\svdo{S}) \: ,
\]
where in the second step we made use of {\em do}-calculus rule~(1): the conditional independencies in the causal chain graph $G$ are preserved when we intervene on some of the variables. Now rule~(3) tells us that we can ignore any interventions from nodes in components further down the causal chain graph as well as those from higher up that are shielded by the direct parents:
\[
P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\svdo{S})
\isequaldono{3} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\svdo{\spa(\tau) \cap S},\svdo{\tau \cap S}) \: .
\]
Rule~(2) then states that conditioning by intervention upon variables higher up in the causal chain graph is equivalent to conditioning by observation:
\[
P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\svdo{\spa(\tau) \cap S},\svdo{\tau \cap S}) \isequaldono{2}
P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S},\svdo{\tau \cap S}) \: .
\]
For a chain component with dependencies induced by a common confounder, rule (3) applies once more and makes that we can ignore the interventions:
\[
P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S},\svdo{\tau \cap S}) = P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S}) \: .
\]
For a chain component with dependencies induced by mutual interactions, rule (2) again applies and allows us to replace conditioning by intervention with conditioning by observation:
\[
P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S},\svdo{\tau \cap S}) = P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{ \spa(\tau) \cap S},\vx_{\tau \cap S})) \: .
\]
\end{proof}

Algorithm~\ref{alg:sampling} provides pseudocode on how to estimate the value function $\val(S)$ by drawing samples from the interventional probability~(\ref{eq:chaininterventional}). It assumes that a user has specified a partial causal ordering of the features, which is translated to a chain graph with components $\chaincomponents$, and for each (non-singleton) component $\tau$ whether or not surplus dependencies are the result of confounding. Other prerequisites include access to the model $f()$, the feature vector $\vx$, (a procedure to sample from) the observational probability distribution $P(\vX)$, and the number of samples $\onder{N}{samples}$.

\begin{algorithm}[t]
	\caption{Compute the value function $\val(S)$ under conditioning by intervention.}
	\label{alg:sampling}
	\begin{algorithmic}[1]
		\Function{ValueFunction}{$S$}
		\For{$k \gets 1 \mbox{~to~} \onder{N}{samples}$}
		\ForAll{$j \gets 1 \mbox{~to~} |\chaincomponents|$} \Comment{run over all chain components in their causal order}
			\If{confounding$(\tau_j)$} 
				\ForAll{$i \in \tau_j \cap \bar{S}$}
					\State Sample $\tilde{x}_i^{(k)} \sim P(X_i|\tilde{\vx}_{\spa(\tau_j) \cap \bar{S}}^{(k)},\vx_{\spa(\tau_j) \cap \bar{S}})$ \Comment{can be drawn independently}
				\EndFor
			\Else
				\State Sample $\tilde{\vx}_{\tau_j \cap \bar{S}}^{(k)} \sim P(\vX_{\tau_j \cap \bar{S}}|\tilde{\vx}_{\spa(\tau_j) \cap \bar{S}}^{(k)},\vx_{\spa(\tau_j) \cap \bar{S}},\vx_{\tau_j \cap S})$ \Comment{e.g., Gibbs sampling}
			\EndIf
		\EndFor
		\EndFor
		\State $\val \gets {\displaystyle {1 \over \onder{N}{samples}} \sum_{k=1}^{\onder{N}{samples}} f(\vx_S,\tilde{\vx}_{\bar{S}}^{(k)})}$
		\State \Return $\val$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Theorem~1 connects to observations made and algorithms proposed in recent papers.
\begin{corollary}
	With all features combined in a single component and all dependencies induced by confounding, as in~\cite{janzing2019feature}, causal Shapley values are equivalent to marginal Shapley values. 
\end{corollary}
\begin{proof}
	With just a single confounded component $\tau$, $\pa(\tau) = \emptyset$ and~(\ref{eq:chaininterventional}) reduces to $P(\vX_{\bar{S}})$.
%	{\em Do}-calculus rule~(3) yields $P(\vX_{\bar{S}}|\svdo{S}) = P(\vX_{\bar{S}})$ for all subsets $S$, i.e., as if all features are independent.
\end{proof}
\begin{corollary}
	With all features combined in a single component and all dependencies induced by mutual interactions, causal Shapley values are equivalent to conditional Shapley values as proposed in~\cite{aas2019explaining}.
\end{corollary}
\begin{proof}
	With just a single non-confounded component $\tau$, $\pa(\tau) = \emptyset$ and~(\ref{eq:chaininterventional}) reduces to $P(\vX_{\bar{S}}|\vx_{S})$.
\end{proof}
\begin{corollary}
	When we only take into account permutations that match the causal ordering and assume that all dependencies within chain components are induced by mutual interactions, the resulting asymmetric causal Shapley values are equivalent to the asymmetric conditional Shapley values as defined in~\cite{frye2019asymmetric}.
\end{corollary}
\begin{proof}
	Following~\cite{frye2019asymmetric}, asymmetric Shapley values only include those permutations $\perm$ for which $i \prec_\perm j$ for all known ancestors $i$ of descendants $j$ in the causal graph. For those permutations, we are guaranteed to have $\tau \prec_G \tau'$ for all $\tau, \tau' \in \mathcal{T}$ such that $\tau \cap S \neq \emptyset, \tau' \cap \bar{S} \neq \emptyset$. That is, the chain components that contain features from $S$ are never later in the causal ordering of the chain graph $G$ than those that contain features from $\bar{S}$. We then have
	\[
	P(\vX_{\bar{S}}|\vx_S) = \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{S}) = \prod_{\tau \in \chaincomponents} P(\vX_{\tau \cap \bar{S}}|\vX_{\spa(\tau)  \cap \bar{S}},\vx_{\spa(\tau) \cap S},\vx_{\tau \cap S}) = P(\vX_{\bar{S}}|\svdo{S}) \: ,
	\]
	where in the last step we used interventional formula~(\ref{eq:chaininterventional}) in combination with the fact that $\onder{\chaincomponents}{confounding} = \emptyset$.
\end{proof}

\section{Additional illustrations on the bike rental data}

Figure~\ref{fig:sinaplots} shows sina plots for asymmetric conditional Shapley values (left) and marginal Shapley values (right), to be compared with the sina plots for symmetric causal Shapley values in Figure~3 of the main text. In this case, the sina plots for asymmetric causal Shapley values are virtually indistinguishable from those for the asymmetric conditional Shapley values.

It can be seen that the marginal Shapley values strongly focus on temperature, largely ignoring the seasonal variables. The asymmetric Shapley values do the opposite: they focus on the seasonal variables, in particular {\em cosyear} and put much less emphasis on the temperature variables.

\begin{figure}[t]
	\centering
	\begin{minipage}{.49\linewidth}
	\includegraphics[width=\textwidth]{figures/sina_plot_asymmetric.pdf}
	\end{minipage}
	\begin{minipage}{.49\linewidth}
	\includegraphics[width=\textwidth]{figures/sina_plot_marginal.pdf}
	\end{minipage}
	\caption{Sina plots of asymmetric (conditional) Shapley values (left) and marginal Shapley values (right). See Figure~3 in the main text for further details.}
	\label{fig:sinaplots}
\end{figure}

\section{Comparing symmetric and asymmetric Shapley values on the XOR problem}

We consider the standard XOR problem with binary features $X_1$ and $X_2$ and binary output $Y$:
\begin{center}
\begin{tabular}{cc|c}
	$X_1$ & $X_2$ & Y \\ \hline
	0     & 0     & 0 \\
	0     & 1     & 1 \\
	1     & 0     & 1 \\
	1     & 1     & 0
\end{tabular}
\end{center}
We generate a dataset of $n$ samples by drawing features and corresponding outputs with probabilities $p_{ij} = P(X_1=i,X_2=j)$. We will choose $p_{00} = p_{11} = \frac{1}{4} (1 + \epsilon)$ and $p_{01} = p_{10} = \frac{1}{4} (1 - \epsilon)$. With $\epsilon > 0$, the probability of the two features having the same values is larger than the probability of them having different values. $\hat{p}_{ij}$ is the same probability estimated from the data, e.g., by computing the frequencies of the four input combinations. We train a neural network on the generated data, which yields a function $\hat{f}(X_1,X_2)$ hopefully closely approximating the correct XOR function.

We will now compute the various Shapley values for the data point $(X_1,X_2) = (0,0)$. The value functions with all features either `out-of-coalition' or `in-coalition' are the same for all Shapley values:
\begin{eqnarray*}
\nu(\{\}) & = & \expectation\left[f(\vX)\right] = \sum_{i,j} \hat{p}_{ij} \hat{f}(i,j) \approx \frac{1}{2} (1 - \epsilon) \\
\nu(\{1,2\}) & = & \hat{f}(0,0) \approx 0 \: ,
\end{eqnarray*}
where we use the convention that the Shapley values computed from the fitted probabilities and learned neural network appear before the $\approx$-sign, and those that we obtain when the fitted probabilities equal the probabilities used to generate the data and when the learned neural network equals the XOR function after the $\approx$-sign.

The value functions for the case that one input is `in-coalition' and the other `out-of-coalition' does depend on the type of Shapley value under consideration. For the marginal Shapley values we get
\begin{eqnarray}
\nu(\{1\}) & = & \expectation\left[f(0,X_2)\right] = \sum_{j} \left(\sum_i \hat{p}_{ij}\right) \hat{f}(0,j) \nonumber \approx {1 \over 2} \\
\nu(\{2\}) & = & \expectation\left[f(X_1,0)\right] = \sum_{i} \left(\sum_j \hat{p}_{ij}\right) \hat{f}(i,0) \approx \frac{1}{2} \: ,
\label{eq:marginal}
\end{eqnarray}
yet for the conditional Shapley values
\begin{eqnarray}
\nu(\{1\}) & = & \expectation\left[f(0,X_2)|X_1=0\right] = \sum_{j} \frac{\hat{p}_{0j}}{\sum_{i} \hat{p}_{ij}} \hat{f}(0,j) \approx \frac{1}{2}(1-\epsilon) \nonumber \\
\nu(\{2\}) & = & \expectation\left[f(X_1,0)|X_2=0\right] = \sum_{i} \frac{\hat{p}_{i0}}{\sum_{j} \hat{p}_{ij}} \hat{f}(i,0) \approx \frac{1}{2}(1-\epsilon) \: .
\label{eq:conditional}
\end{eqnarray}
The value functions for the causal Shapley values depend on the presumed causal model that generates the dependencies. In case the dependencies are assumed to be the result of confounding, we get the value functions in~(\ref{eq:marginal}) as for the marginal Shapley values and when the dependencies are assumed to be the result of mutual interaction the value functions in~(\ref{eq:conditional}) as for the conditional Shapley values. The more interesting case is when we assume a causal chain, e.g., $X_1 \rightarrow X_2$:
\begin{eqnarray}
\nu(\{1\}) & = & \expectation\left[f(0,X_2)|\dodo(X_1=0)\right] = \expectation\left[f(0,X_2)|X_1=0\right] = \sum_{j} \frac{\hat{p}_{0j}}{\sum_{i} \hat{p}_{ij}} \hat{f}(0,j) \approx \frac{1}{2}(1-\epsilon) \nonumber \\
\nu(\{2\}) & = & \expectation\left[f(X_1,0)|\dodo(X_2=0)\right] =\expectation\left[f(X_1,0)\right] = \sum_{i} \left(\sum_j \hat{p}_{ij}\right) \hat{f}(i,0) \approx \frac{1}{2} \: ,
\label{eq:causal}
\end{eqnarray}
and the same with indices 1 and 2 interchanged for the causal chain $X_2 \rightarrow X_1$.


\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/XOR_plot.pdf}
	\caption{The conditional symmetric, causal symmetric and causal asymmetric Shapley values of data point $(X_1,X_2) = (0,0)$ under assumption of causal chain $X_1 \rightarrow X_2$ for different $\epsilon$. The bars indicate the mean Shapley value and standard deviation of 100 runs with a neural network trained on 100 data points generated according to $\epsilon$ with as label the outcome of the XOR-function. The red line indicates the theoretical Shapley values as denoted by identical, symmetric causal and asymmetric.
	}
	\label{fig:xor_plot}
\end{figure}


Given these value functions, we can now compute the various Shapley values. For marginal and symmetric Shapley values we have
\begin{eqnarray}
\phi_1 & = & \frac{1}{2}[\nu(\{1\}) - \nu(\{\})] + \frac{1}{2}[\nu(\{1,2\}) - \nu(\{2\})] \nonumber \\
\phi_2 & = & \frac{1}{2}[\nu(\{2\}) - \nu(\{\})] + \frac{1}{2}[\nu(\{1,2\}) - \nu(\{1\}]) \: , \nonumber
\end{eqnarray}
whereas for asymmetric Shapley values, assuming the causal chain $X_1 \rightarrow X_2$,
\begin{eqnarray}
\phi_1 & = & \nu(\{1\}) - \nu(\{\}) \nonumber \\
\phi_2 & = & \nu(\{1,2\}) - \nu(\{1\}) \: , \nonumber
\end{eqnarray}
and the same with indices 1 and 2 interchanged for the causal chain $X_2 \rightarrow X_1$.

With the expressions above, we can compute the various Shapley values based on a learned neural network and the actual frequencies of the generated feature combinations and compare those with the theoretical values obtained when the estimated frequencies equal the probabilities used to generate the data and the neural network indeed managed to learn the XOR function. For the latter we distinguish the following cases.
\begin{description}
	\item[identical:] $\phi_1 = \phi_2 \approx \frac{1}{4} \epsilon - \frac{1}{4}$. This applies to marginal, symmetric conditional, symmetric causal assuming confounding, symmetric causal assuming mutual interaction.
	\item[symmetric causal:] $\phi_1 \approx -\frac{1}{4}$ and $\phi_2 \approx \frac{1}{2}\epsilon - \frac{1}{4}$ assuming the causal chain $X_1 \rightarrow X_2$ and vice versa for $X_1 \rightarrow X_2$.
	\item[asymmetric:] $\phi_1 \approx 0$ and $\phi_2 \approx \frac{1}{2}\epsilon - \frac{1}{2}$ assuming the causal chain $X_1 \rightarrow X_2$ and vice versa for $X_1 \rightarrow X_2$. These apply both to asymmetric conditional and asymmetric causal.
\end{description}

In this example, symmetric causal Shapley values are clearly to be preferred over asymmetric causal Shapley values. Inserting a causal link with zero strength ($\epsilon=0$), asymmetric Shapley values jump from the symmetric $\phi_1 = \phi_2 \approx -\frac{1}{4}$ to the completely asymmetric $\phi_1 \approx 0$ and $\phi_2 \approx - \frac{1}{2}$, assigning all credit to the second variable, even though the first feature in reality does not affect the second feature at all. Symmetric Shapley values, on the other hand, are insensitive to the insertion of a causal link with zero strength: in the limit $\epsilon \rightarrow 0$ symmetric causal Shapley values correctly converge to marginal Shapley values as also follows from the experimental results in figure~\ref{fig:xor_plot}.


\bibliography{shapleyrefs}
\bibliographystyle{plain}


\end{document}