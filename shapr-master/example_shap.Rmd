---
title: "R Notebook"
output: html_notebook
---

```{r}
library(xgboost)
#library(shapr)
pkgload::load_all()

```


```{r}


data("Boston", package = "MASS")

x_var <- c("lstat", "rm", "dis", "indus")
y_var <- "medv"

ind_x_test <- 1:6
x_train <- as.matrix(Boston[-ind_x_test, x_var])
y_train <- Boston[-ind_x_test, y_var]
x_test <- as.matrix(Boston[ind_x_test, x_var])

# Looking at the dependence between the features
cor(x_train)
```

```{r}
# Fitting a basic xgboost model to the training data
model <- xgboost(
  data = x_train,
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Prepare the data for explanation
explainer <- shapr(x_train, model)

# Specifying the phi_0, i.e. the expected prediction without any features
p <- mean(y_train)
```


```{r}
# Computing the actual Shapley values with kernelSHAP accounting for feature dependence using
# the empirical (conditional) distribution approach with bandwidth parameter sigma = 0.1 (default)
explanation <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(c(1:4)),
  confounding = TRUE
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
# Finally we plot the resulting explanations
plot(explanation)
```

```{r}
# Computing the actual Shapley values with kernelSHAP accounting for feature dependence using
# the empirical (conditional) distribution approach with bandwidth parameter sigma = 0.1 (default)
explanation <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(4,2,3,1),
  confounding = c(FALSE,FALSE,FALSE,FALSE)
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
# Finally we plot the resulting explanations
plot(explanation)
```
```{r}
#setwd("../gcd")
#source("gcd_preprocessing.R")
load("gcd/gcd_data_bin.Rdata")
N <- dim(data)[1]

set.seed(0)
trainIndex <- caret::createDataPartition(data$credit_risk, p = .99, list = FALSE, times = 1)
x_var <- c("status", "duration", "amount", "savings", "age", "housing", "sex")
y_var <- "credit_risk"
x_train <- as.matrix(data[trainIndex,x_var])
y_train <- data[trainIndex,y_var]
x_test <- as.matrix(data[-trainIndex,x_var])
# Looking at the dependence between the features
cor(x_train)
```
```{r}
# Fitting a basic xgboost model to the training data
model <- xgboost(
  data = x_train,
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Prepare the data for explanation
explainer <- shapr(x_train, model)

# Specifying the phi_0, i.e. the expected prediction without any features
p <- mean(y_train)
```
```{r}
explanation <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(c(1:7)),
  confounding = TRUE
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
# Finally we plot the resulting explanations
plot(explanation)
```
```{r}
explanation <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(7,5,c(1,3,6),c(2,4)),
  confounding = c(FALSE,FALSE,FALSE,FALSE)
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
# Finally we plot the resulting explanations
plot(explanation)
```
```{r}
bike <- read.csv("bike/bike-sharing-daily.csv")
bike$trend <- as.integer(difftime(bike$dteday, min(as.Date(bike$dteday)))+1)/24
bike$cosyear <- cospi(bike$trend/365*2)
bike$sinyear <- sinpi(bike$trend/365*2)
bike$temp <- bike$temp * (39 - (-8)) + (-8)
bike$atemp <- bike$atemp * (50 - (-16)) + (-16)
bike$cosweek <- cospi(bike$weekday/7*2)
bike$sinweek <- sinpi(bike$weekday/7*2)
bike$windspeed <- 67 * bike$windspeed
bike$hum <- 100 * bike$hum
#x_var <- c("trend","cosyear","sinyear","holiday","cosweek","sinweek","workingday","temp","atemp","windspeed","hum")
#x_var <- c("cosyear","sinyear","cosweek","sinweek","temp","windspeed","hum")
x_var <- c("trend","cosyear","sinyear","temp","atemp","windspeed","hum")
y_var <- "cnt"
set.seed(2)
trainIndex <- caret::createDataPartition(bike$cnt, p = .8, list = FALSE, times = 1)
x_train <- as.matrix(bike[trainIndex,x_var])
y_train <- bike[trainIndex,y_var]
y_train <- y_train - mean(y_train)
x_test <- as.matrix(bike[-trainIndex,x_var])
#dummy <- sample(1:dim(x_test)[1])
#x_test <- x_test[dummy[1:8],]
print(x_test)
dim(x_test)
```
```{r}
# Fitting a basic xgboost model to the training data
model <- xgboost(
  data = x_train,
  label = y_train,
  nround = 100,
  verbose = FALSE
)

# Prepare the data for explanation
explainer <- shapr(x_train, model)

# Specifying the phi_0, i.e. the expected prediction without any features
p <- mean(y_train)
```
```{r}
explanation <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(c(1:7)),
  confounding = FALSE
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
# Finally we plot the resulting explanations
#plot(explanation)
```
```{r}
#install.packages("ggforce")
for (ii in seq(7)){
  plot(as.matrix(x_test)[,ii],as.matrix(explanation$dt)[,ii+1])
  }
```


```{r}
explanation2 <- explain(
  x_test,
  approach = "causal",
  explainer = explainer,
  prediction_zero = p,
  ordering = list(1,c(2,3),c(4,5,6,7)),
  confounding = c(TRUE,TRUE,FALSE)
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation2$dt)
# Finally we plot the resulting explanations
#plot(explanation)
```

```{r}
for (ii in seq(7)){
  plot(as.matrix(explanation$dt)[,ii+1],as.matrix(explanation2$dt)[,ii+1])
  }
```

```{r}
set.seed(2)
rnorm(1)
R.version
```

